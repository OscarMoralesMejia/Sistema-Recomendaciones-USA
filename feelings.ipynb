{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo de Machine Learning para el sistema de analisis de sentimientos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importar todas las librerias a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Rhamer\\Desktop\\Henry\\entorno\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Rhamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rhamer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "from transformers import DistilBertTokenizer,DistilBertForSequenceClassification\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, SequentialSampler\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love there korean rice cake</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2014-03-16 05:10:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good very good</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2014-02-21 00:29:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>they make korean traditional food very properly</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-01-30 19:38:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>short ribs are very delicious</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2019-11-30 22:23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great food and prices the portions are large</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-07-15 13:11:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281037</th>\n",
       "      <td>this place took over osaka ramen we tried the ...</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2017-07-02 19:41:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281038</th>\n",
       "      <td>delicious ramen clean dinning room and good se...</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2020-11-05 01:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281039</th>\n",
       "      <td>rich broth soft meat and fresh noodles</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2021-09-12 06:52:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281040</th>\n",
       "      <td>best food ever</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2021-11-17 01:27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281041</th>\n",
       "      <td>nothing extraordinary</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2017-07-02 14:15:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281042 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0                             love there korean rice cake   \n",
       "1                                          good very good   \n",
       "2         they make korean traditional food very properly   \n",
       "3                           short ribs are very delicious   \n",
       "4            great food and prices the portions are large   \n",
       "...                                                   ...   \n",
       "281037  this place took over osaka ramen we tried the ...   \n",
       "281038  delicious ramen clean dinning room and good se...   \n",
       "281039             rich broth soft meat and fresh noodles   \n",
       "281040                                     best food ever   \n",
       "281041                              nothing extraordinary   \n",
       "\n",
       "                                      gmap_id               fecha  \n",
       "0       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2014-03-16 05:10:15  \n",
       "1       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2014-02-21 00:29:23  \n",
       "2       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-01-30 19:38:55  \n",
       "3       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2019-11-30 22:23:42  \n",
       "4       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-07-15 13:11:12  \n",
       "...                                       ...                 ...  \n",
       "281037  0x808fe955b0beae57:0xb3159fe6572670c3 2017-07-02 19:41:03  \n",
       "281038  0x808fe955b0beae57:0xb3159fe6572670c3 2020-11-05 01:30:44  \n",
       "281039  0x808fe955b0beae57:0xb3159fe6572670c3 2021-09-12 06:52:46  \n",
       "281040  0x808fe955b0beae57:0xb3159fe6572670c3 2021-11-17 01:27:55  \n",
       "281041  0x808fe955b0beae57:0xb3159fe6572670c3 2017-07-02 14:15:15  \n",
       "\n",
       "[281042 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cargar dataset de reviews\n",
    "reviews = pd.read_parquet('reviews_final.parquet')\n",
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocesamiento de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# # Tokenizar las reseñas\n",
    "# encoding = tokenizer(reviews['text'].tolist(), truncation=True, padding=True, return_tensors='pt', max_length=512)\n",
    "\n",
    "# input_ids = encoding['input_ids']\n",
    "# attention_masks = encoding['attention_mask']\n",
    "\n",
    "# # Crear tensores de datos\n",
    "# data = TensorDataset(input_ids, attention_masks)\n",
    "\n",
    "# # Crear DataLoader\n",
    "# batch_size = 16\n",
    "# dataloader = DataLoader(data, sampler=SequentialSampler(data), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lematizacion y tokenizacion de las reviews para ser analizados por el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Utilizando la libreria nltk, nlp y torch se realiza el preprocesamiento de los datos incluidos en la review</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de abreviaturas específicas a eliminar.\n",
    "abbreviations = {\n",
    "    'asap', 'btw', 'brb', 'idk', 'imo', 'imho', 'lol', 'omg', 'tbh', 'ttyl', 'smh', 'np', 'ftw', 'fyi', 'bff',\n",
    "    'lmao', 'rofl', 'xoxo', 'afaik', 'fml', 'gtg', 'hmu', 'icymi', 'ikr', 'ily', 'jk', 'nvm', 'roflmao', 'smh',\n",
    "    'stfu', 'tldr', 'wtf', 'yw', 'bday', 'gr8', 'thx', 'pls', 'u', 'ur', 'yolo','xlent', 'xclnt', 'xlnt','xlbs', 'asap', 'btw', 'xd'\n",
    "}\n",
    "\n",
    "# Definir las stop_words.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(abbreviations)\n",
    "\n",
    "# Funcion para remover las stop_words.\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() not in stop_words])\n",
    "\n",
    "# funcion para limpiar las reviews.\n",
    "def clean_review(review):\n",
    "    # Eliminar stopwords.\n",
    "    review = remove_stopwords(review)\n",
    "    # Eliminar palabras con caracteres repetitivos (ej: \"aaaa\", \"ahh\").\n",
    "    review = ' '.join([word for word in review.split() if not re.search(r'(.)\\1{2,}', word)])\n",
    "    return review\n",
    "\n",
    "# funcion para la lematizacion de las reviews.\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar la lematización y la limpieza\n",
    "reviews['text'] = reviews['text'].apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['text'] = reviews['text'].apply(clean_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar reseñas con menos de min_word_count palabras\n",
    "min_word_count = 5\n",
    "filtered_reviews = reviews[reviews['text'].apply(lambda x: len(x.split()) >= min_word_count)].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>love korean rice cake</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2014-03-16 05:10:15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>good good</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2014-02-21 00:29:23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>make korean traditional food properly</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-01-30 19:38:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>short rib delicious</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2019-11-30 22:23:42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>great food price portion large</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-07-15 13:11:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281037</th>\n",
       "      <td>place take osaka raman try black garlic raman ...</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2017-07-02 19:41:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281038</th>\n",
       "      <td>delicious raman clean din room good service</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2020-11-05 01:30:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281039</th>\n",
       "      <td>rich broth soft meat fresh noodle</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2021-09-12 06:52:46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281040</th>\n",
       "      <td>good food ever</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2021-11-17 01:27:55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281041</th>\n",
       "      <td>nothing extraordinary</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2017-07-02 14:15:15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>281042 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0                                   love korean rice cake   \n",
       "1                                               good good   \n",
       "2                   make korean traditional food properly   \n",
       "3                                     short rib delicious   \n",
       "4                          great food price portion large   \n",
       "...                                                   ...   \n",
       "281037  place take osaka raman try black garlic raman ...   \n",
       "281038        delicious raman clean din room good service   \n",
       "281039                  rich broth soft meat fresh noodle   \n",
       "281040                                     good food ever   \n",
       "281041                              nothing extraordinary   \n",
       "\n",
       "                                      gmap_id               fecha  \n",
       "0       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2014-03-16 05:10:15  \n",
       "1       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2014-02-21 00:29:23  \n",
       "2       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-01-30 19:38:55  \n",
       "3       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2019-11-30 22:23:42  \n",
       "4       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-07-15 13:11:12  \n",
       "...                                       ...                 ...  \n",
       "281037  0x808fe955b0beae57:0xb3159fe6572670c3 2017-07-02 19:41:03  \n",
       "281038  0x808fe955b0beae57:0xb3159fe6572670c3 2020-11-05 01:30:44  \n",
       "281039  0x808fe955b0beae57:0xb3159fe6572670c3 2021-09-12 06:52:46  \n",
       "281040  0x808fe955b0beae57:0xb3159fe6572670c3 2021-11-17 01:27:55  \n",
       "281041  0x808fe955b0beae57:0xb3159fe6572670c3 2017-07-02 14:15:15  \n",
       "\n",
       "[281042 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear y cargar el modelo preentrenado de DistilBert para la clasificacion de los reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8d472c4914c46d7b0e90bd9e9e6f757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e2afc54575145918740201bd66d6f47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Descargar y cargar el modelo preentrenado para análisis de sentimientos\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Tokenizar las reseñas\n",
    "encodings = tokenizer(list(filtered_reviews['text']), truncation=True, padding=True, max_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crear el DataLoader\n",
    "input_ids = torch.tensor(encodings['input_ids'])\n",
    "attention_mask = torch.tensor(encodings['attention_mask'])\n",
    "dataset = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(dataset, sampler=SequentialSampler(dataset), batch_size=32)\n",
    "\n",
    "# Mover el modelo a la GPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear funcion y cargar resultado en el dataframe en una nueva columna llamada sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>fecha</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make korean traditional food properly</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-01-30 19:38:55</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great food price portion large</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-07-15 13:11:12</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chicken sandwich delicious definitely twist fl...</td>\n",
       "      <td>0x80dd2b4c8555edb7:0xfc33d65c4bdbef42</td>\n",
       "      <td>2013-12-21 05:26:13</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love place fry garlic chicken crispy savory al...</td>\n",
       "      <td>0x80dd2b4c8555edb7:0xfc33d65c4bdbef42</td>\n",
       "      <td>2022-09-20 07:51:08</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delicious variety food good place go either qu...</td>\n",
       "      <td>0x80c2d765f8c90a3d:0x16afb75943e7ad50</td>\n",
       "      <td>2013-06-06 18:41:37</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0              make korean traditional food properly   \n",
       "1                     great food price portion large   \n",
       "2  chicken sandwich delicious definitely twist fl...   \n",
       "3  love place fry garlic chicken crispy savory al...   \n",
       "4  delicious variety food good place go either qu...   \n",
       "\n",
       "                                 gmap_id               fecha sentiment  \n",
       "0  0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-01-30 19:38:55  positive  \n",
       "1  0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-07-15 13:11:12  positive  \n",
       "2  0x80dd2b4c8555edb7:0xfc33d65c4bdbef42 2013-12-21 05:26:13  positive  \n",
       "3  0x80dd2b4c8555edb7:0xfc33d65c4bdbef42 2022-09-20 07:51:08  positive  \n",
       "4  0x80c2d765f8c90a3d:0x16afb75943e7ad50 2013-06-06 18:41:37  positive  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Funcion para analizar el texto.\n",
    "def predict_sentiment(dataloader, model, device):\n",
    "    model.eval()\n",
    "    sentiments = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids, batch_attention_mask = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch_input_ids, attention_mask=batch_attention_mask)\n",
    "        \n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=1).flatten()\n",
    "        sentiments.extend(predictions.cpu().numpy())\n",
    "    \n",
    "    return sentiments\n",
    "\n",
    "# Realizar predicciones\n",
    "sentiments = predict_sentiment(dataloader, model, device)\n",
    "\n",
    "# Añadir las predicciones al DataFrame\n",
    "filtered_reviews['sentiment'] = ['positive' if sentiment == 1 else 'negative' for sentiment in sentiments]\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame con las predicciones\n",
    "filtered_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear archivo de tipo parquet y crear tabla en base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews.to_parquet('review_final_final.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>gmap_id</th>\n",
       "      <th>fecha</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make korean traditional food properly</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-01-30 19:38:55</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great food price portion large</td>\n",
       "      <td>0x80c2c778e3b73d33:0xbdc58662a4a97d49</td>\n",
       "      <td>2016-07-15 13:11:12</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chicken sandwich delicious definitely twist fl...</td>\n",
       "      <td>0x80dd2b4c8555edb7:0xfc33d65c4bdbef42</td>\n",
       "      <td>2013-12-21 05:26:13</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>love place fry garlic chicken crispy savory al...</td>\n",
       "      <td>0x80dd2b4c8555edb7:0xfc33d65c4bdbef42</td>\n",
       "      <td>2022-09-20 07:51:08</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>delicious variety food good place go either qu...</td>\n",
       "      <td>0x80c2d765f8c90a3d:0x16afb75943e7ad50</td>\n",
       "      <td>2013-06-06 18:41:37</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169904</th>\n",
       "      <td>maybe order delivery noodle hard eat soup room...</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2014-09-04 00:38:44</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169905</th>\n",
       "      <td>great food staff kind gentleman help tonight g...</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2018-06-05 03:31:51</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169906</th>\n",
       "      <td>place take osaka raman try black garlic raman ...</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2017-07-02 19:41:03</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169907</th>\n",
       "      <td>delicious raman clean din room good service</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2020-11-05 01:30:44</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169908</th>\n",
       "      <td>rich broth soft meat fresh noodle</td>\n",
       "      <td>0x808fe955b0beae57:0xb3159fe6572670c3</td>\n",
       "      <td>2021-09-12 06:52:46</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>169909 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0                   make korean traditional food properly   \n",
       "1                          great food price portion large   \n",
       "2       chicken sandwich delicious definitely twist fl...   \n",
       "3       love place fry garlic chicken crispy savory al...   \n",
       "4       delicious variety food good place go either qu...   \n",
       "...                                                   ...   \n",
       "169904  maybe order delivery noodle hard eat soup room...   \n",
       "169905  great food staff kind gentleman help tonight g...   \n",
       "169906  place take osaka raman try black garlic raman ...   \n",
       "169907        delicious raman clean din room good service   \n",
       "169908                  rich broth soft meat fresh noodle   \n",
       "\n",
       "                                      gmap_id               fecha sentiment  \n",
       "0       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-01-30 19:38:55  positive  \n",
       "1       0x80c2c778e3b73d33:0xbdc58662a4a97d49 2016-07-15 13:11:12  positive  \n",
       "2       0x80dd2b4c8555edb7:0xfc33d65c4bdbef42 2013-12-21 05:26:13  positive  \n",
       "3       0x80dd2b4c8555edb7:0xfc33d65c4bdbef42 2022-09-20 07:51:08  positive  \n",
       "4       0x80c2d765f8c90a3d:0x16afb75943e7ad50 2013-06-06 18:41:37  positive  \n",
       "...                                       ...                 ...       ...  \n",
       "169904  0x808fe955b0beae57:0xb3159fe6572670c3 2014-09-04 00:38:44  positive  \n",
       "169905  0x808fe955b0beae57:0xb3159fe6572670c3 2018-06-05 03:31:51  positive  \n",
       "169906  0x808fe955b0beae57:0xb3159fe6572670c3 2017-07-02 19:41:03  negative  \n",
       "169907  0x808fe955b0beae57:0xb3159fe6572670c3 2020-11-05 01:30:44  positive  \n",
       "169908  0x808fe955b0beae57:0xb3159fe6572670c3 2021-09-12 06:52:46  positive  \n",
       "\n",
       "[169909 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# labels = [1, 0]  # 1 = positiva, 0 = negativa\n",
    "\n",
    "\n",
    "# # Dividir los datos\n",
    "# train_texts, val_texts, train_labels, val_labels = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Crear el dataset\n",
    "# class ReviewDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, encodings, labels):\n",
    "#         self.encodings = encodings\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "#         item['labels'] = torch.tensor(self.labels[idx])\n",
    "#         return item\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "# train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "# val_dataset = ReviewDataset(val_encodings, val_labels)\n",
    "\n",
    "# # Configurar y entrenar el modelo\n",
    "# model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir='./results',          \n",
    "#     num_train_epochs=3,              \n",
    "#     per_device_train_batch_size=16,  \n",
    "#     per_device_eval_batch_size=16,   \n",
    "#     warmup_steps=500,                \n",
    "#     weight_decay=0.01,               \n",
    "#     logging_dir='./logs',            \n",
    "#     logging_steps=10,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=model,                         \n",
    "#     args=training_args,                  \n",
    "#     train_dataset=train_dataset,         \n",
    "#     eval_dataset=val_dataset             \n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entorno",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
